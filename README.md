# ðŸš€ Databricks ELT Pipeline â€” Delta Live Tables (DLT) | Medallion Architecture

## ðŸ“Œ Project Description

This project demonstrates an end-to-end **ELT data engineering pipeline** built on **Databricks** using **Delta Live Tables (DLT)** and **PySpark**.  
The objective was to simulate real-world streaming data ingestion and build a pipeline that processes raw sales records into business-ready insights using the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**.

Data used in this pipeline includes:
- Sales data arriving from **two regions (East & West)** â†’ incremental streaming inserts
- Product dimension data (with updates â†’ price change, name change)
- Customer dimension data (with updates â†’ location change, name correction)

Even though the dataset is small (created using SQL inserts), it simulates:
- **Streaming / incremental ingestion**
- **Slowly Changing Dimensions (CDC updates)**


---



## âœ… What I Built

### ðŸ”¹ Bronze Layer â€” Ingestion
- Ingested raw regional sales tables (`sales_east`, `sales_west`) using  
  âœ… `dlt.create_streaming_table()`  
  âœ… `@dlt.append_flow()`  
- Applied **data quality rules** (`expect_all_or_drop`) on ingestion.

### ðŸ”¹ Silver Layer â€” Transformations
- Cleaned/enriched data (casting, computed columns like `total_sales`)
- Applied **Auto-CDC** (`dlt.create_auto_cdc_flow`) to handle:
  - Product price changes (SCD Type 1)
  - Customer region/name updates (SCD Type 2)
- Ensured only valid incremental changes are processed.

### ðŸ”¹ Gold Layer â€” Business Output
- Built a star-schema style model:
  - `dim_products`
  - `dim_customers`
  - `fact_sales`
- Created a reporting layer that shows:
  - **Total sales grouped by Region and Product Category**

---

## ðŸ› ï¸ Tools & Tech Used

| Component | Technology Used |
|----------|-----------------|
| Platform | Databricks |
| Storage Format | Delta Lake (ACID transactions, versioning) |
| ETL Framework | **Delta Live Tables (DLT)** |
| Language | PySpark / SQL |
| Concepts Used | Streaming Ingestion, AutoCDC, SCD Type 1 & 2, Medallion Architecture |
| Version Control | Git + GitHub + Databricks Repos |

---

## ðŸŒŸ Key Learnings

- How to build a **streaming ELT pipeline** using Databricks DLT
- How to apply **data quality expectations**
- How **CDC + SCD Type 1 & 2** are handled automatically by DLT
- How to transform raw data into **business analytics outputs**

---

> This project demonstrates hands-on experience in Data Engineering using Databricks, Delta Lake, ETL automation, and streaming pipelines similar to what real companies build in production.

