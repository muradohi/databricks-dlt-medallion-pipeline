# Databricks notebook source
# MAGIC %md
# MAGIC ### Exploratory Notebook
# MAGIC
# MAGIC Using this notebook to explore the data generated by the pipeline.
# MAGIC

# COMMAND ----------

from pyspark.sql.functions import *

# COMMAND ----------

df_sales = spark.read.table("dlt_catalog.sch_dlt.fact_sales")
df_cust = spark.read.table("dlt_catalog.sch_dlt.dim_customers")
df_prod = spark.read.table("dlt_catalog.sch_dlt.dim_products")

# COMMAND ----------

df_sales.limit(5).display()

# COMMAND ----------

df_sales_alias = df_sales.alias("sales")
df_cust_alias = df_cust.alias("cust")
df_prod_alias = df_prod.alias("prod")

df_joined = df_sales_alias.join(
    df_cust_alias,
    df_sales_alias["customer_id"] == df_cust_alias["customer_id"],
    "inner"
).join(
    df_prod_alias,
    df_sales_alias["product_id"] == df_prod_alias["product_id"],
    "inner"
)
display(df_joined)

# COMMAND ----------

df_select = df_joined.select(
    "sales.customer_id",
    "cust.customer_name",
    "sales.product_id",
    "prod.product_name",
    "cust.region",
    "sales.quantity",
    "sales.amount",
    "sales.sale_timestamp",
    "sales.total_sales"
)
display(df_select)

# COMMAND ----------

df_filtered = df_select.filter((df_select["region"] == "East") & (df_select["product_name"]== "Laptop" ))
display(df_filtered)

# COMMAND ----------

df_year = df_select.withColumn("year", year(df_select["sale_timestamp"]))
display(df_year)

# COMMAND ----------

df_year = df_select.withColumn("hour", hour(df_select["sale_timestamp"]))\
    .withColumn("day", dayofmonth(df_select["sale_timestamp"]))\
    .withColumn("month", month(df_select["sale_timestamp"]))\
    .withColumn("year", year(df_select["sale_timestamp"]))
display(df_year)

# COMMAND ----------

df_year.write.format("delta")\
    .mode("append")\
    .save("/Volumes/delta_files/default/delta_vol")

# COMMAND ----------

df = spark.read.format("delta").load("/Volumes/delta_files/default/delta_vol")
df.display()
